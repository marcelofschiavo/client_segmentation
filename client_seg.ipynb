{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNiImJzlUQ9pGAofS+Rx/H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcelofschiavo/client_segmentation/blob/main/client_seg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Segmentação de Clientes (Clustering Não-Supervisionado)\n",
        "\n",
        "Objetivo: Uma empresa como a @empresa não pode tratar todos os seus milhões de clientes da mesma forma. Este projeto busca usar Machine Learning Não-Supervisionado (Clustering) para identificar perfis (personas) de clientes com base em seu comportamento de compra.\n",
        "\n",
        "Problema de Negócio: \"Quais são os principais perfis de clientes que compram nossos produtos? Como podemos criar ações de marketing e retenção personalizadas para cada grupo?\"\n",
        "\n",
        "Metodologia:\n",
        "\n",
        "ETL Simulado: Criação de um dataset fictício (mas realista) de comportamento de clientes.\n",
        "\n",
        "Pré-Processamento (StandardScaler): Etapa crucial do clustering para padronizar a escala das variáveis.\n",
        "\n",
        "Descoberta (Método do Cotovelo): Encontrar o número \"ideal\" de clusters (K).\n",
        "\n",
        "Modelagem (K-Means): Treinar o modelo de clustering para agrupar os clientes.\n",
        "\n",
        "Análise de Personas (Psicologia): Interpretar as médias de cada cluster para criar \"personas\" de negócio.\n",
        "\n",
        "Bibliotecas: pandas (manipulação), sklearn (para StandardScaler, KMeans, PCA) e seaborn (visualização)."
      ],
      "metadata": {
        "id": "ApfKgIxmX9w8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgvBzCCOX33L"
      },
      "outputs": [],
      "source": [
        "# Importar bibliotecas fundamentais\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Bibliotecas de Visualização (EDA)\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Bibliotecas de ML (Clustering)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA # Para visualização 2D\n",
        "\n",
        "# Configuração\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "# Usamos seed(42) para garantir que nossos dados aleatórios sejam sempre os mesmos\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Bibliotecas carregadas com sucesso.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Etapa 1: Criação dos Dados (ETL Simulado)\n",
        "\n",
        "Vamos criar um dataset fictício de 300 clientes. Para segmentar clientes, usamos um modelo comportamental famoso chamado RFM (Recência, Frequência, Valor Monetário), mas com um toque extra de psicologia:\n",
        "\n",
        "dias_desde_ultima_compra: (Recência) Cliente ativo ou \"dormindo\"?\n",
        "\n",
        "gasto_total_ano: (Monetário) Cliente gasta muito ou pouco?\n",
        "\n",
        "frequencia_compras_ano: (Frequência) Cliente compra sempre ou raramente?\n",
        "\n",
        "percentual_desconto_usado: (Comportamental) É um \"caçador de promoções\"?"
      ],
      "metadata": {
        "id": "Jb6Lk06mYD-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ETAPA 1: Criando dataset fictício...\")\n",
        "\n",
        "num_clientes = 300\n",
        "\n",
        "data = {\n",
        "    'id_cliente': range(1, num_clientes + 1),\n",
        "    'dias_desde_ultima_compra': np.random.randint(1, 365, num_clientes),\n",
        "    'gasto_total_ano': np.random.normal(loc=1000, scale=400, size=num_clientes).clip(50), # média 1000\n",
        "    'frequencia_compras_ano': np.random.randint(1, 50, num_clientes),\n",
        "    'percentual_desconto_usado': np.random.rand(num_clientes).clip(0, 0.5) # De 0% a 50%\n",
        "}\n",
        "\n",
        "df_clientes = pd.DataFrame(data)\n",
        "\n",
        "# Para tornar os clusters mais interessantes, vamos \"forçar\" alguns perfis:\n",
        "# Cluster 0 (VIPs): 50 clientes que gastam muito, compram sempre, desconto baixo\n",
        "df_clientes.loc[0:50, 'gasto_total_ano'] = np.random.normal(loc=3500, scale=500, size=51)\n",
        "df_clientes.loc[0:50, 'frequencia_compras_ano'] = np.random.randint(40, 70, 51)\n",
        "df_clientes.loc[0:50, 'dias_desde_ultima_compra'] = np.random.randint(1, 30, 51)\n",
        "\n",
        "# Cluster 1 (Em Risco): 50 clientes que gastam pouco, não compram há muito tempo\n",
        "df_clientes.loc[51:100, 'gasto_total_ano'] = np.random.normal(loc=500, scale=100, size=50)\n",
        "df_clientes.loc[51:100, 'frequencia_compras_ano'] = np.random.randint(1, 5, 50)\n",
        "df_clientes.loc[51:100, 'dias_desde_ultima_compra'] = np.random.randint(200, 365, 50)\n",
        "\n",
        "# O resto (101-299) são clientes \"médios\" ou \"caçadores de promoção\"\n",
        "df_clientes.loc[101:200, 'percentual_desconto_usado'] = np.random.rand(100).clip(0.4, 0.8) # Usam muito desconto\n",
        "\n",
        "print(\"Dataset criado com sucesso:\")\n",
        "print(df_clientes.describe())"
      ],
      "metadata": {
        "id": "5weBm7CxYGfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ETAPA 1: Criando dataset fictício...\")\n",
        "\n",
        "num_clientes = 300\n",
        "\n",
        "data = {\n",
        "    'id_cliente': range(1, num_clientes + 1),\n",
        "    'dias_desde_ultima_compra': np.random.randint(1, 365, num_clientes),\n",
        "    'gasto_total_ano': np.random.normal(loc=1000, scale=400, size=num_clientes).clip(50), # média 1000\n",
        "    'frequencia_compras_ano': np.random.randint(1, 50, num_clientes),\n",
        "    'percentual_desconto_usado': np.random.rand(num_clientes).clip(0, 0.5) # De 0% a 50%\n",
        "}\n",
        "\n",
        "df_clientes = pd.DataFrame(data)\n",
        "\n",
        "# Para tornar os clusters mais interessantes, vamos \"forçar\" alguns perfis:\n",
        "# Cluster 0 (VIPs): 50 clientes que gastam muito, compram sempre, desconto baixo\n",
        "df_clientes.loc[0:50, 'gasto_total_ano'] = np.random.normal(loc=3500, scale=500, size=51)\n",
        "df_clientes.loc[0:50, 'frequencia_compras_ano'] = np.random.randint(40, 70, 51)\n",
        "df_clientes.loc[0:50, 'dias_desde_ultima_compra'] = np.random.randint(1, 30, 51)\n",
        "\n",
        "# Cluster 1 (Em Risco): 50 clientes que gastam pouco, não compram há muito tempo\n",
        "df_clientes.loc[51:100, 'gasto_total_ano'] = np.random.normal(loc=500, scale=100, size=50)\n",
        "df_clientes.loc[51:100, 'frequencia_compras_ano'] = np.random.randint(1, 5, 50)\n",
        "df_clientes.loc[51:100, 'dias_desde_ultima_compra'] = np.random.randint(200, 365, 50)\n",
        "\n",
        "# O resto (101-299) são clientes \"médios\" ou \"caçadores de promoção\"\n",
        "df_clientes.loc[101:200, 'percentual_desconto_usado'] = np.random.rand(100).clip(0.4, 0.8) # Usam muito desconto\n",
        "\n",
        "print(\"Dataset criado com sucesso:\")\n",
        "print(df_clientes.describe())"
      ],
      "metadata": {
        "id": "9snD-W_MYH_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Etapa 2: Pré-Processamento (StandardScaler)\n",
        "\n",
        "Esta é a etapa mais importante do clustering.\n",
        "\n",
        "O K-Means funciona medindo a \"distância\" entre os pontos. A feature gasto_total_ano (valores de 50 a 3500) teria um \"peso\" mil vezes maior que percentual_desconto_usado (valores de 0 a 1). Isso distorceria totalmente nosso modelo.\n",
        "\n",
        "O StandardScaler resolve isso. Ele transforma todas as nossas features para que elas tenham a mesma escala (média 0 e desvio padrão 1), garantindo que todas tenham a mesma importância inicial para o modelo."
      ],
      "metadata": {
        "id": "VqGsIGVcYKwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ETAPA 2: Padronizando a escala dos dados...\")\n",
        "\n",
        "# Selecionamos apenas as colunas que descrevem o comportamento\n",
        "features = ['dias_desde_ultima_compra', 'gasto_total_ano', 'frequencia_compras_ano', 'percentual_desconto_usado']\n",
        "df_features = df_clientes[features]\n",
        "\n",
        "# 1. Instanciar o Scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# 2. Treinar e Transformar os dados\n",
        "# O .fit() aprende a média e o desvio\n",
        "# O .transform() aplica a padronização\n",
        "df_scaled = scaler.fit_transform(df_features)\n",
        "\n",
        "print(\"Dados padronizados (média 0, desvio 1):\")\n",
        "print(df_scaled[:5]) # Mostra as 5 primeiras linhas"
      ],
      "metadata": {
        "id": "RJBCiAiaYMx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Etapa 3: Encontrando o Número Ideal de Clusters (O Método do Cotovelo)\n",
        "\n",
        "Nós não sabemos quantos \"perfis\" de clientes existem. São 2? 3? 5?\n",
        "\n",
        "O \"Método do Cotovelo\" (Elbow Method) nos ajuda a decidir. Vamos rodar o K-Means com vários 'K's (de 1 a 10) e calcular a \"Inércia\" (a soma total das distâncias dos clientes até o centro do seu cluster).\n",
        "\n",
        "Procuramos o ponto no gráfico onde a linha \"quebra\" (o 'cotovelo'). Este é o ponto onde adicionar mais um cluster não traz muito mais benefício (a inércia não cai tanto)."
      ],
      "metadata": {
        "id": "BRsmftY9Ys__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ETAPA 3: Método do Cotovelo (Elbow Method)...\")\n",
        "\n",
        "inertia = []\n",
        "K_range = range(1, 11)\n",
        "\n",
        "for k in K_range:\n",
        "    # n_init=10 evita resultados ruins por sorteio inicial\n",
        "    model = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    model.fit(df_scaled)\n",
        "    inertia.append(model.inertia_)\n",
        "\n",
        "# Plotar o gráfico do cotovelo\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(K_range, inertia, marker='o', linestyle='--')\n",
        "plt.xlabel('Número de Clusters (k)')\n",
        "plt.ylabel('Inércia')\n",
        "plt.title('Método do Cotovelo (Elbow Method)')\n",
        "plt.xticks(K_range)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YhgsKaU2YR_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Etapa 4: Treinamento do Modelo K-Means\n",
        "\n",
        "O gráfico do cotovelo (como forçamos os dados) deve mostrar uma \"quebra\" clara em K=3 ou K=4. Para este exercício, vamos assumir que k=4 é o número ideal de perfis de clientes.\n",
        "\n",
        "Agora vamos treinar o modelo K-Means final com k=4 e usar o .labels_ para \"carimbar\" cada cliente com o seu rótulo de cluster (0, 1, 2, ou 3)."
      ],
      "metadata": {
        "id": "3HL8EIgMYUjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ETAPA 4: Treinando o modelo K-Means final...\")\n",
        "\n",
        "k_ideal = 4\n",
        "\n",
        "# 1. Instanciar o modelo\n",
        "model_kmeans = KMeans(n_clusters=k_ideal, random_state=42, n_init=10)\n",
        "\n",
        "# 2. Treinar o modelo com os dados padronizados\n",
        "model_kmeans.fit(df_scaled)\n",
        "\n",
        "# 3. Pegar os rótulos (os grupos) e salvar no DataFrame original\n",
        "df_clientes['cluster'] = model_kmeans.labels_\n",
        "\n",
        "print(\"DataFrame final com os clusters atribuídos:\")\n",
        "print(df_clientes.head())\n",
        "\n",
        "# Verificar quantos clientes em cada cluster\n",
        "print(\"\\nContagem de clientes por cluster:\")\n",
        "print(df_clientes['cluster'].value_counts())"
      ],
      "metadata": {
        "id": "v2HKlfuvYVEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Etapa 5: Análise dos Clusters (A Psicologia / O Insight)\n",
        "\n",
        "O modelo técnico está pronto. Agora vem o trabalho do Cientista de Dados Comportamental: Interpretar o que esses clusters significam.\n",
        "\n",
        "Vamos calcular a média de cada feature (gasto_total, frequencia, etc.) para cada cluster e entender o \"perfil\" de cada grupo.\n",
        "\n",
        "Para visualizar os grupos (já que não podemos plotar 4 dimensões), usaremos PCA (Análise de Componentes Principais) para \"comprimir\" nossas 4 features em 2 eixos (PC1 e PC2) e plotar em um gráfico 2D."
      ],
      "metadata": {
        "id": "xhsGmXB3YWM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ETAPA 5: Interpretando os perfis dos clusters...\")\n",
        "\n",
        "# 1. Análise de Perfil (A Média de cada cluster)\n",
        "# Nós usamos 'df_features' (o original, não padronizado) para interpretar\n",
        "# os resultados em valores reais (ex: R$ 3500)\n",
        "cluster_profile = df_clientes.groupby('cluster')[features].mean()\n",
        "\n",
        "print(\"--- Perfil Médio de Cada Cluster ---\")\n",
        "print(cluster_profile)\n",
        "\n",
        "# 2. Visualização 2D com PCA\n",
        "print(\"\\nGerando visualização 2D dos clusters...\")\n",
        "\n",
        "# Instanciar o PCA\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# Aplicar o PCA nos dados padronizados\n",
        "df_pca = pca.fit_transform(df_scaled)\n",
        "\n",
        "# Salvar os componentes no DataFrame\n",
        "df_clientes['pc1'] = df_pca[:, 0]\n",
        "df_clientes['pc2'] = df_pca[:, 1]\n",
        "\n",
        "# Plotar o scatterplot\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.scatterplot(x='pc1', y='pc2', hue='cluster', data=df_clientes, palette='deep', s=100, alpha=0.8)\n",
        "plt.title('Visualização 2D dos Clusters de Clientes (via PCA)')\n",
        "plt.xlabel('Componente Principal 1 (PC1)')\n",
        "plt.ylabel('Componente Principal 2 (PC2)')\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TVj-DGlMYYox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Etapa 6: Conclusão e Personas (A Entrega de Negócio)\n",
        "\n",
        "Com base na tabela cluster_profile (as médias de cada grupo), traduzimos os números em Personas acionáveis para a equipe de Marketing da @mepresa.\n",
        "\n",
        "(Nota: Os perfis abaixo são uma interpretação dos dados fictícios que criamos. Os seus podem variar um pouco.)\n",
        "\n",
        "Cluster 0 (Ex: Gasto Alto, Frequência Alta, Recência Baixa):\n",
        "\n",
        "Persona: \"Clientes VIP\"\n",
        "\n",
        "Comportamento: Nossos melhores clientes. Gastam muito, compram sempre e compraram recentemente.\n",
        "\n",
        "Ação de Negócio (Marketing): Programas de fidelidade, acesso antecipado a lançamentos, brindes de luxo. Não enviar descontos (eles não precisam).\n",
        "\n",
        "Cluster 1 (Ex: Gasto Baixo, Frequência Baixa, Recência Alta):\n",
        "\n",
        "Persona: \"Clientes em Risco (Churn)\"\n",
        "\n",
        "Comportamento: Eram clientes, mas gastaram pouco e não compram há muito tempo (Recência alta).\n",
        "\n",
        "Ação de Negócio (Retenção): Campanha de reengajamento \"Sentimos sua falta!\" com um cupom de desconto agressivo para reativá-los.\n",
        "\n",
        "Cluster 2 (Ex: Gasto Médio, Frequência Média, Alto Desconto):\n",
        "\n",
        "Persona: \"Caçadores de Promoção\"\n",
        "\n",
        "Comportamento: Clientes que só compram quando há promoções. O percentual_desconto_usado deles é o mais alto.\n",
        "\n",
        "Ação de Negócio (Financeiro): Excluir este grupo das campanhas de desconto mais caras. Enviar apenas promoções de \"leve 3 pague 2\" (foco em volume, não em margem).\n",
        "\n",
        "Cluster 3 (Ex: Gasto Médio, Frequência Baixa, Recência Média):\n",
        "\n",
        "Persona: \"Clientes Regulares / Oportunidade\"\n",
        "\n",
        "Comportamento: O cliente \"médio\". Compra de vez em quando, gasta um valor razoável.\n",
        "\n",
        "Ação de Negócio (Upsell): Campanha de cross-sell e upsell. Sugerir produtos de novas categorias (Ex: \"Vimos que você adora maquiagem, que tal conhecer nossa linha de pele?\")."
      ],
      "metadata": {
        "id": "qe83WEptYaJJ"
      }
    }
  ]
}